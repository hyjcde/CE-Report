% !TEX root = ../thesis.tex

\chapter{Literature Review} \label{chp:literature}

This chapter aims to systematically review the current state of research across four critical domains through a critical examination of existing literature, thereby precisely identifying the theoretical and technological gaps that this research seeks to address and elucidating the innovative starting point of this investigation. These four domains are: Digital Twin maturity models, Retrieval-Augmented Generation (RAG), Large Language Model (LLM) Agent paradigms, and classical decision and control architectures in physical systems.

\section{Digital Twin Models and Applications}

Digital Twins (DT), as a core enabling technology for Cyber-Physical Systems, have undergone extensive research and continuous evolution in both academic and industrial communities. The theoretical foundations of this field can be traced back to early concepts proposed by NASA for addressing aircraft health management challenges, which emphasized the three fundamental components: physical entities, virtual models, and data connections \cite{glaessgen2012digital}. Building upon this foundation, Grieves proposed a three-dimensional conceptual model consisting of physical space entities, virtual space models, and the connections between them, providing a concise yet profoundly influential starting point for theoretical development in this field \cite{grieves2014digital}.

To make this concept more operationally viable for guiding complex industrial practices, Tao Fei and his team proposed a more comprehensive and systematic five-dimensional Digital Twin model \cite{tao2018digital}. This model significantly expanded upon the three-dimensional framework by explicitly defining five core dimensions: Physical Entity, Virtual Entity, Digital Twin Data, Services System, and Connection. Tao's five-dimensional model has gained widespread recognition in domains such as intelligent manufacturing due to its systematic emphasis on the central role of ``services'' and ``data'' in realizing Digital Twin functionality, providing clear theoretical guidance for constructing fully functional Digital Twin systems.

\subsection{Engineering Maturity Frameworks}

At the engineering application level, industry and research institutions have proposed a series of maturity models designed to assess and guide Digital Twin implementation. The Fraunhofer Institute's five-stage model categorizes Digital Twin development from basic data collection through full autonomous operation, emphasizing progressive capabilities in data integration, model sophistication, and real-time responsiveness \cite{kritzinger2018digital}. Lockheed Martin's five-level model focuses on the integration complexity and autonomy levels, ranging from standalone simulations to fully integrated, self-managing systems \cite{rasheed2020digital}.

The International Organization for Standardization's ISO 23247 series standards provide normative guidance for Digital Twin architecture and data exchange in manufacturing, marking the technology's progression toward standardization and large-scale application \cite{iso23247}. These standards establish common terminology, reference architectures, and interoperability requirements that facilitate systematic implementation across diverse industrial contexts.

However, upon deeper examination, these theoretical models and maturity standards share a common limitation when viewed from the perspective of future intelligent applications. Whether considering the component structure of Tao's five-dimensional model or the engineering capability classifications in various maturity models, their assessment dimensions are fundamentally approached from the perspectives of system builders and data managers. Their core focus is describing what constitutes a Digital Twin system and assessing its engineering capabilities—essentially evaluating how well a Digital Twin is ``built'' rather than determining its suitability as a decision environment for AI agents of specific cognitive complexity.

\subsection{Functional Classification Gaps}

For instance, while Tao's model identifies the ``Services System'' as a critical dimension, it does not differentiate the cognitive complexity of services themselves. A ``descriptive'' service providing historical data queries presents vastly different requirements for AI agent reasoning capabilities compared to a ``predictive'' service requiring complex simulation model invocation for what-if scenario analysis. Current models cannot provide direct guidance for AI researchers facing questions such as: ``For the complex strategic planning agent I am developing, at which functional level of Digital Twin environment should testing be conducted?''

This limitation reveals a critical gap in existing frameworks: the absence of a functional classification system oriented toward AI applications. While engineering maturity models excel at assessing implementation readiness and technical capabilities, they lack a cognitive task-oriented perspective that would enable systematic evaluation of Digital Twin environments as cognitive substrates for AI decision-making.

Recent research has begun to address this gap through cognitive-oriented taxonomies that classify Digital Twins based on their decision-support capabilities rather than their technical implementation characteristics \cite{jones2020cognitive, negri2017review}. These emerging frameworks recognize three primary functional categories: monitoring-oriented twins that support descriptive analytics, prediction-oriented twins that enable scenario analysis, and optimization-oriented twins that facilitate autonomous decision-making. However, these classifications remain preliminary and lack the systematic theoretical development necessary for principled AI system design.

\section{RAG: Principles and Limitations}

Retrieval-Augmented Generation (RAG) has emerged as the standard paradigm for mitigating knowledge staleness and factual hallucination issues in Large Language Models (LLMs). The core mechanism involves utilizing user queries to retrieve relevant information fragments from external knowledge bases (typically text vector databases) prior to generation, injecting this information as context into the prompt provided to the LLM \cite{lewis2020retrieval, karpukhin2020dense}.

This ``retrieve-first, generate-second'' pattern has achieved tremendous success in open-domain question answering and enterprise knowledge base scenarios dominated by unstructured text. By providing LLMs with immediate, relevant external knowledge, RAG significantly enhances response relevance and factual accuracy while enabling access to information beyond the model's training cutoff \cite{guu2020retrieval}.

\subsection{RAG Architecture and Implementations}

Standard RAG implementations typically employ dense passage retrieval using pre-trained encoders such as DPR (Dense Passage Retrieval) or more recent contrastive learning approaches \cite{karpukhin2020dense, izacard2021leveraging}. These systems convert documents into dense vector representations that capture semantic content, enabling similarity-based retrieval for user queries. Advanced implementations incorporate re-ranking mechanisms, fusion approaches for combining multiple retrieved passages, and iterative retrieval strategies that refine searches based on generated content \cite{yu2022generate}.

Recent developments have extended RAG beyond simple text retrieval to incorporate structured knowledge from knowledge graphs, tables, and databases. FiD (Fusion-in-Decoder) demonstrates how multiple retrieved passages can be processed jointly to generate more comprehensive responses \cite{izacard2021leveraging}. RAG-Token and RAG-Sequence variants explore different integration strategies for retrieved content, showing how retrieval can be performed at multiple granularities during generation \cite{lewis2020retrieval}.

Self-RAG and other adaptive retrieval approaches represent efforts to make retrieval more selective and context-aware, enabling models to determine when external knowledge is needed and how to best utilize retrieved information \cite{asai2023self}. These approaches address limitations of fixed retrieval strategies that may retrieve irrelevant information or fail to retrieve when external knowledge would be beneficial.

\subsection{Paradigm Mismatch in Physical Environments}

However, when attempting to directly apply this paradigm, which has achieved remarkable success in the textual world, to Digital Twin environments, a fundamental ``paradigm mismatch'' emerges. Digital Twin environments feature data forms that extend far beyond unstructured text, with their core value residing precisely in the structured, dynamic, and multimodal nature of their data representations.

For structured data such as Building Information Models (BIM) or relational databases, information is encoded within precise table structures and entity relationships. Standard RAG's vectorization-based retrieval destroys these structural relationships, rendering it incapable of supporting precise operations such as ``query all pipes with diameter greater than a specific value and material of a particular type'' \cite{tang2019retrieving}.

For high-frequency time-series data generated by Internet of Things (IoT) sensors, value lies in dynamic characteristics such as trends, cycles, and anomalies rather than isolated data points. Traditional text encoders cannot effectively capture these temporal patterns, and vector similarity may not correspond to meaningful temporal relationships \cite{yue2022ts2vec}.

Digital Twins commonly contain multimodal data including engineering drawings, infrared images, and physical simulation visualizations that cannot be effectively processed by traditional text-based retrieval systems. Recent multimodal RAG approaches attempt to address these limitations through vision-language models and multimodal embedding spaces, but these remain preliminary solutions that do not fully address the complexity of physical world data \cite{chen2022murag}.

\subsection{Structured Query Integration Challenges}

The integration of structured query capabilities with natural language understanding presents significant technical challenges. Traditional database query languages such as SQL are designed for precise, deterministic retrieval but lack the flexibility to handle natural language ambiguity and context dependence. Conversely, LLMs excel at interpreting natural language but struggle with the precision required for structured data operations \cite{scholak2021duorat}.

Text-to-SQL generation approaches attempt to bridge this gap by training models to convert natural language questions into structured queries. However, these approaches face limitations in handling complex multi-table joins, temporal reasoning, and domain-specific terminology common in Digital Twin environments \cite{yu2018spider}. The semantic gap between natural language expressions and formal query constructs remains a significant challenge.

Recent work on semantic parsing for structured knowledge bases shows promise for more sophisticated integration approaches. These methods learn to map natural language to formal meaning representations that can be executed against structured data sources \cite{berant2013semantic}. However, adapting these approaches to the dynamic, multimodal nature of Digital Twin data requires substantial extensions to current methodologies.

\section{LLM Agents: Evolution and Challenges}

The emergence of Large Language Models (LLMs) has catalyzed not only a revolution in natural language processing but also a paradigmatic shift toward transforming LLMs from passive text generators into active, goal-oriented intelligent agents. This evolution centers on endowing LLMs with the capabilities to plan and utilize external tools, fundamentally expanding their operational scope from text generation to autonomous action \cite{mialon2023augmented}.

\subsection{From Language Models to Autonomous Agents}

Early explorations, such as Chain-of-Thought (CoT) prompting, revealed LLMs' potential for multi-step reasoning by encouraging explicit articulation of reasoning processes \cite{wei2022chain}. This work demonstrated that LLMs could be prompted to break down complex problems into manageable steps, significantly improving performance on mathematical reasoning and logical problem-solving tasks.

Building upon this foundation, several landmark agent frameworks emerged that systematically constructed ``think-act'' feedback loops. The ReAct framework ingeniously interweaved reasoning and acting within the same context, enabling LLMs to generate interpretable action trajectories by explicitly articulating their ``thought processes'' before deciding which tools to invoke \cite{yao2022react}. This approach significantly enhanced task planning robustness and debuggability by making the reasoning process transparent and verifiable.

Toolformer pursued an alternative approach by fine-tuning models during pre-training to spontaneously learn when, where, and how to invoke APIs during text generation, thus more intrinsically integrating tool usage capabilities into the model itself \cite{schick2023toolformer}. This approach demonstrated that tool usage could be learned as an emergent capability rather than explicitly programmed behavior.

Autonomous agent projects exemplified by AutoGPT and AgentGPT pushed this paradigm to new heights, enabling systems to autonomously decompose high-level user goals, formulate multi-step plans, execute tool invocations, engage in self-reflection and critique, and manage complex projects using both short-term and long-term memory \cite{richards2023autogpt}. These agents demonstrated remarkable success in purely digital environments, skillfully orchestrating web browsers, search engines, code interpreters, and various APIs to complete complex tasks including market research, software development, and itinerary planning.

\subsection{Success in Digital Domains}

The success of LLM agents in digital environments has been particularly notable in domains where tools and interfaces are designed for programmatic access. Software development agents can utilize code repositories, integrated development environments, and testing frameworks to write, debug, and deploy code \cite{nijkamp2022codegen}. Research assistants can navigate academic databases, synthesize literature, and generate comprehensive reports by orchestrating multiple information sources \cite{press2022measuring}.

Web automation agents demonstrate sophisticated capabilities in navigating complex web interfaces, extracting information from multiple sources, and completing multi-step online tasks \cite{deng2023mind2web}. These systems can handle dynamic web content, adapt to interface changes, and maintain task context across extended interaction sessions.

The development of sophisticated memory and planning mechanisms has enabled agents to handle increasingly complex, long-term objectives. Hierarchical planning approaches enable agents to decompose high-level goals into manageable sub-tasks, while episodic memory systems allow agents to learn from past experiences and adapt their strategies over time \cite{sumers2023cognitive}.

\subsection{The Cognitive-Physical Gap}

However, when attention shifts from the purely digital world to complex physical environments, the foundational assumptions underlying current LLM agent paradigms begin to falter, revealing a profound ``cognitive-physical gap.'' This gap is not merely a technical interface incompatibility issue but stems from fundamental mismatches between LLM cognitive patterns and the intrinsic laws of the physical world, manifesting across three critical dimensions.

\subsubsection{Physical Commonsense and Grounding Difficulties}

LLMs' knowledge derives from textual statistics rather than embodied physical interaction, resulting in a lack of causally grounded physical commonsense. While an LLM may know textual definitions of ``force'' and ``temperature,'' it cannot truly understand that applying force generates acceleration or that heating an object requires energy and involves temporal delays \cite{bisk2020experience}. Consequently, when planning for physical systems, LLM-generated strategies may appear syntactically and logically reasonable but prove physically meaningless or even dangerous.

This grounding problem is exacerbated by the symbolic nature of LLM representations, which lack the sensorimotor foundations that enable humans to develop intuitive physical understanding \cite{harnad1990symbol}. Recent work on physics-informed language models attempts to address these limitations by incorporating physical principles into training objectives, but these approaches remain preliminary \cite{lu2021physics}.

\subsubsection{Complex Physical Model Utilization Barriers}

Physical world ``tools'' are vastly more complex than Web APIs that return JSON-formatted data. For instance, a Finite Element Analysis (FEA) or Computational Fluid Dynamics (CFD) simulation model requires structured mesh files and complex boundary condition configurations as inputs, involves computationally intensive and time-consuming execution processes, and produces multi-dimensional data fields or visualization outputs requiring professional expertise to interpret \cite{hughes2012finite}.

Current agent frameworks' simple ``tool selection-API invocation'' patterns are entirely inadequate for handling such ``heavy-duty tools'' that demand deep understanding and complex interaction protocols. The gap between natural language instructions and formal simulation setup requirements represents a significant challenge for LLM-based systems \cite{li2023apibank}.

Recent work on code generation for scientific computing shows promise but remains limited in scope and requires substantial domain expertise to implement effectively \cite{chen2021evaluating}. The challenge of automatically configuring complex simulation environments based on natural language descriptions remains largely unsolved.

\subsubsection{Safety and Real-Time Execution Assurance Deficits}

LLM reasoning processes are inherently slow and non-deterministic, with complex reasoning potentially requiring seconds or longer, and results not being entirely consistent across iterations. This conflicts sharply with physical systems, especially robotics or industrial control systems, which demand high-frequency, deterministic, and safety-first operation principles \cite{thrun2002probabilistic}.

In dynamic environments, robots cannot wait for LLMs to complete ``deliberate'' planning but must respond to obstacles within millisecond timeframes. Current agent frameworks universally lack reliable mechanisms to coordinate LLM slow cognitive planning with the rapid safety responses required by the physical world. Any instance of ``hallucination'' or planning error could result in equipment damage or safety incidents \cite{amodei2016concrete}.

Existing safety mechanisms in LLM systems focus primarily on content safety rather than action safety in physical environments. The development of real-time safety monitors and intervention systems for LLM-controlled physical systems remains an open research challenge \cite{pecka2014safe}.

\subsection{Limitations of Current Integration Approaches}

Recent attempts to integrate LLMs with physical systems have largely focused on narrow applications or simplified scenarios that do not fully address the fundamental challenges outlined above. Robot control applications typically rely on high-level task decomposition while delegating low-level control to traditional systems, avoiding the need for real-time LLM decision-making \cite{ahn2022saycan}.

Simulation-based approaches attempt to provide safe environments for LLM agents to learn physical reasoning, but the reality gap between simulation and physical deployment remains significant \cite{tobin2017domain}. While these approaches provide valuable research platforms, they do not address the fundamental challenges of deploying LLM agents in real physical environments.

Multi-modal LLMs that can process visual and textual inputs represent important progress toward physical world understanding, but these systems still lack the temporal reasoning and causal understanding necessary for effective physical world interaction \cite{alayrac2022flamingo}. The integration of multimodal perception with physical action remains an open challenge.

\section{Research Gap Analysis}

Through systematic review of these four critical technology domains, this chapter reveals a clear and important research gap. The Digital Twin domain provides ideal carriers and environments for digital representation of the physical world, but existing maturity models primarily approach from engineering implementation perspectives, universally lacking functional classification frameworks oriented toward artificial intelligence decision-making requirements.

\subsection{Integration Challenges}

Retrieval-Augmented Generation (RAG) and Large Language Model (LLM) Agent technologies bring powerful cognitive and reasoning capabilities, but their existing paradigms face ``paradigm mismatch'' problems when applied to the physical world—incompatibility with structured, multimodal, dynamic data—as well as ``physical gap'' problems including lack of physical commonsense, inability to utilize complex engineering models, and difficulty ensuring safe execution.

Classical physical system control architectures guarantee high safety and real-time performance, but their symbolic logic-dependent cognitive cores suffer from fundamental bottlenecks of rigid knowledge representation and limited planning capabilities. The fundamental tension between the flexibility required for intelligent behavior and the determinism required for safe physical operation remains largely unresolved.

\subsection{Absence of Systematic Solutions}

Currently, no research work has proposed a unified architecture capable of systematically integrating the advantages of these four domains. Existing solutions either focus on improvements to individual technical points or perform simple model combinations, failing to fundamentally propose comprehensive solutions that simultaneously address the three core challenges LLMs face in the physical world: data grounding, complex model utilization, and safe decision execution.

The literature reveals several partial solutions that address subsets of these challenges. Multimodal LLMs improve sensory grounding but do not address temporal reasoning or safety constraints. Tool-augmented agents provide frameworks for external tool usage but are not designed for the complexity of physical simulation tools. Hierarchical control systems ensure safety but lack the flexibility for novel problem-solving.

\subsection{Evaluation and Benchmarking Gaps}

Furthermore, the absence of standardized evaluation methodologies for integrated LLM-physical world systems represents a significant methodological gap. Traditional NLP metrics are inadequate for assessing physical world performance, while engineering metrics for physical systems do not capture the nuanced requirements of cognitive agents. The development of appropriate evaluation frameworks remains an open challenge.

The complexity of physical world environments makes it difficult to create reproducible benchmarks that capture the full range of challenges these systems must address. Unlike text-based tasks where datasets can be easily shared and compared, physical world evaluation requires consideration of safety, environmental variability, and the high cost of real-world testing.

\subsection{Future Research Directions}

The identified gaps point toward several important research directions. The development of cognitive-oriented Digital Twin frameworks that can serve as effective interfaces between symbolic reasoning and physical reality represents a crucial need. Similarly, the extension of RAG paradigms to handle structured, temporal, and multimodal data requires fundamental advances in retrieval and integration methodologies.

The creation of safe, real-time integration frameworks for LLM-based reasoning in physical systems represents perhaps the most critical challenge. This requires not only technical solutions for latency and safety but also theoretical frameworks for understanding the appropriate roles of different types of computation in physical world intelligence.

The development of comprehensive evaluation methodologies that can assess both cognitive capabilities and physical world performance across diverse domains will be essential for advancing the field. These methodologies must balance the need for rigorous assessment with the practical constraints of physical world testing.

\section{Chapter Summary}

This literature review has systematically examined four critical technology domains to establish the theoretical foundation for addressing LLM-physical world integration challenges. Through comprehensive analysis of Digital Twin maturity models, RAG paradigms, LLM agent frameworks, and classical control architectures, several key insights emerge.

The review reveals that while each domain has achieved significant individual advances, fundamental gaps remain in their integration for physical world applications. Digital Twin frameworks provide sophisticated world representations but lack cognitive-oriented design principles. RAG systems excel with textual knowledge but struggle with structured, multimodal physical data. LLM agents demonstrate impressive reasoning in digital domains but face fundamental challenges when deployed in physical environments. Classical control architectures ensure safety and reliability but lack the flexibility for autonomous problem-solving in novel situations.

The convergence of these technologies presents unprecedented opportunities for creating intelligent physical world systems, but realizing this potential requires systematic approaches that address the fundamental challenges revealed by this review. The absence of principled integration frameworks, appropriate evaluation methodologies, and safety-assured operation paradigms represents significant barriers to progress.

The identified research gaps establish the foundation for the following chapters, which will present novel approaches to LLM-Digital Twin integration that attempt to address these fundamental challenges through systematic architectural design and comprehensive empirical evaluation. The literature review demonstrates both the necessity and the opportunity for advancing beyond current approaches toward more effective integration of cognitive capabilities with physical world operation.