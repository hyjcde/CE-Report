
@misc{liu_swin_2021,
	title = {Swin {Transformer}: {Hierarchical} {Vision} {Transformer} using {Shifted} {Windows}},
	shorttitle = {Swin {Transformer}},
	url = {http://arxiv.org/abs/2103.14030},
	doi = {10.48550/arXiv.2103.14030},
	abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with {\textbackslash}textbf\{S\}hifted {\textbackslash}textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at{\textasciitilde}{\textbackslash}url\{https://github.com/microsoft/Swin-Transformer\}.},
	urldate = {2025-04-17},
	publisher = {arXiv},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	month = aug,
	year = {2021},
	note = {arXiv:2103.14030 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/huangyijun/Zotero/storage/ZJYRBLIX/Liu 等 - 2021 - Swin Transformer Hierarchical Vision Transformer using Shifted Windows.pdf:application/pdf;Snapshot:/Users/huangyijun/Zotero/storage/EY6A2CU5/2103.html:text/html},
}

@article{le_goallec_using_2022,
	title = {Using deep learning to predict abdominal age from liver and pancreas magnetic resonance images},
	volume = {13},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-29525-9},
	doi = {10.1038/s41467-022-29525-9},
	abstract = {Abstract
            
              With age, the prevalence of diseases such as fatty liver disease, cirrhosis, and type two diabetes increases. Approaches to both predict abdominal age and identify risk factors for accelerated abdominal age may ultimately lead to advances that will delay the onset of these diseases. We build an abdominal age predictor by training convolutional neural networks to predict abdominal age (or “AbdAge”) from 45,552 liver magnetic resonance images [MRIs] and 36,784 pancreas MRIs (R-Squared = 73.3 ± 0.6; mean absolute error = 2.94 ± 0.03 years). Attention maps show that the prediction is driven by both liver and pancreas anatomical features, and surrounding organs and tissue. Abdominal aging is a complex trait, partially heritable (h\_g
              2
               = 26.3 ± 1.9\%), and associated with 16 genetic loci (e.g. in 
              PLEKHA1
              and
              EFEMP1
              ), biomarkers (e.g body impedance), clinical phenotypes (e.g, chest pain), diseases (e.g. hypertension), environmental (e.g smoking), and socioeconomic (e.g education, income) factors.},
	language = {en},
	number = {1},
	urldate = {2025-04-19},
	journal = {Nature Communications},
	author = {Le Goallec, Alan and Diai, Samuel and Collin, Sasha and Prost, Jean-Baptiste and Vincent, Théo and Patel, Chirag J.},
	month = apr,
	year = {2022},
	pages = {1979},
	file = {PDF:/Users/huangyijun/Zotero/storage/BYSJE6L6/Le Goallec 等 - 2022 - Using deep learning to predict abdominal age from liver and pancreas magnetic resonance images.pdf:application/pdf},
}

@article{yin_deep_2023,
	title = {Deep learning for pancreatic diseases based on endoscopic ultrasound: {A} systematic review},
	volume = {174},
	issn = {1386-5056},
	shorttitle = {Deep learning for pancreatic diseases based on endoscopic ultrasound},
	url = {https://www.sciencedirect.com/science/article/pii/S138650562300062X},
	doi = {10.1016/j.ijmedinf.2023.105044},
	abstract = {Background and aims
Endoscopic ultrasonography (EUS) is one of the main examinations in pancreatic diseases. A series of the studies reported the application of deep learning (DL)-assisted EUS in the diagnosis of pancreatic diseases. This systematic review is to evaluate the role of DL algorithms in assisting EUS diagnosis of pancreatic diseases.
Methods
Literature search were conducted in PubMed and Semantic Scholar databases. Studies that developed DL models for pancreatic diseases based on EUS were eligible for inclusion. This review was conducted in accordance with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines and quality assessment of the included studies was performed according to the IJMEDI checklist.
Results
A total of 23 studies were enrolled into this systematic review, which could be categorized into three groups according to computer vision tasks: classification, detection and segmentation. Seventeen studies focused on the classification task, among which five studies developed simple neural network (NN) models while twelve studies constructed convolutional NN (CNN) models. Three studies were concerned the detection task and five studies were the segmentation task, all based on CNN architectures. All models presented in the studies performed well based on EUS images, videos or voice. According to the IJMEDI checklist, six studies were recognized as high-grade quality, with scores beyond 35 points.
Conclusions
DL algorithms show great potential in EUS images/videos/voice for pancreatic diseases. However, there is room for improvement such as sample sizes, multi-center cooperation, data preprocessing, model interpretability, and code sharing.},
	urldate = {2025-04-20},
	journal = {International Journal of Medical Informatics},
	author = {Yin, Minyue and Liu, Lu and Gao, Jingwen and Lin, Jiaxi and Qu, Shuting and Xu, Wei and Liu, Xiaolin and Xu, Chunfang and Zhu, Jinzhou},
	month = jun,
	year = {2023},
	keywords = {Convolutional neural networks, Deep learning, Endoscopic ultrasonography, Pancreatic diseases, Systematic review},
	pages = {105044},
	file = {ScienceDirect Snapshot:/Users/huangyijun/Zotero/storage/ENFGLSFL/S138650562300062X.html:text/html},
}

@article{chen_pancreatic_2023,
	title = {Pancreatic {Cancer} {Detection} on {CT} {Scans} with {Deep} {Learning}: {A} {Nationwide} {Population}-based {Study}},
	volume = {306},
	issn = {0033-8419, 1527-1315},
	shorttitle = {Pancreatic {Cancer} {Detection} on {CT} {Scans} with {Deep} {Learning}},
	url = {http://pubs.rsna.org/doi/10.1148/radiol.220152},
	doi = {10.1148/radiol.220152},
	language = {en},
	number = {1},
	urldate = {2025-04-20},
	journal = {Radiology},
	author = {Chen, Po-Ting and Wu, Tinghui and Wang, Pochuan and Chang, Dawei and Liu, Kao-Lang and Wu, Ming-Shiang and Roth, Holger R. and Lee, Po-Chang and Liao, Wei-Chih and Wang, Weichung},
	month = jan,
	year = {2023},
	pages = {172--182},
	file = {Full Text PDF:/Users/huangyijun/Zotero/storage/K6AI4S45/Chen 等 - 2023 - Pancreatic Cancer Detection on CT Scans with Deep Learning A Nationwide Population-based Study.pdf:application/pdf},
}

@article{si_fully_2021,
	title = {Fully end-to-end deep-learning-based diagnosis of pancreatic tumors},
	volume = {11},
	issn = {1838-7640},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7778580/},
	doi = {10.7150/thno.52508},
	abstract = {Artificial intelligence can facilitate clinical decision making by considering massive amounts of medical imaging data. Various algorithms have been implemented for different clinical applications. Accurate diagnosis and treatment require reliable and interpretable data. For pancreatic tumor diagnosis, only 58.5\% of images from the First Affiliated Hospital and the Second Affiliated Hospital, Zhejiang University School of Medicine are used, increasing labor and time costs to manually filter out images not directly used by the diagnostic model., Methods: This study used a training dataset of 143,945 dynamic contrast-enhanced CT images of the abdomen from 319 patients. The proposed model contained four stages: image screening, pancreas location, pancreas segmentation, and pancreatic tumor diagnosis., Results: We established a fully end-to-end deep-learning model for diagnosing pancreatic tumors and proposing treatment. The model considers original abdominal CT images without any manual preprocessing. Our artificial-intelligence-based system achieved an area under the curve of 0.871 and a F1 score of 88.5\% using an independent testing dataset containing 107,036 clinical CT images from 347 patients. The average accuracy for all tumor types was 82.7\%, and the independent accuracies of identifying intraductal papillary mucinous neoplasm and pancreatic ductal adenocarcinoma were 100\% and 87.6\%, respectively. The average test time per patient was 18.6 s, compared with at least 8 min for manual reviewing. Furthermore, the model provided a transparent and interpretable diagnosis by producing saliency maps highlighting the regions relevant to its decision., Conclusions: The proposed model can potentially deliver efficient and accurate preoperative diagnoses that could aid the surgical management of pancreatic tumor.},
	number = {4},
	urldate = {2025-04-20},
	journal = {Theranostics},
	author = {Si, Ke and Xue, Ying and Yu, Xiazhen and Zhu, Xinpei and Li, Qinghai and Gong, Wei and Liang, Tingbo and Duan, Shumin},
	month = jan,
	year = {2021},
	pmid = {33408793},
	pmcid = {PMC7778580},
	pages = {1982--1990},
	file = {Full Text PDF:/Users/huangyijun/Zotero/storage/Y2R2BTSW/Si 等 - 2021 - Fully end-to-end deep-learning-based diagnosis of pancreatic tumors.pdf:application/pdf},
}

@article{liu_deep_2020,
	title = {Deep learning to distinguish pancreatic cancer tissue from non-cancerous pancreatic tissue: a retrospective study with cross-racial external validation},
	volume = {2},
	issn = {25897500},
	shorttitle = {Deep learning to distinguish pancreatic cancer tissue from non-cancerous pancreatic tissue},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2589750020300789},
	doi = {10.1016/S2589-7500(20)30078-9},
	abstract = {Background The diagnostic performance of CT for pancreatic cancer is interpreter-dependent, and approximately 40\% of tumours smaller than 2 cm evade detection. Convolutional neural networks (CNNs) have shown promise in image analysis, but the networks’ potential for pancreatic cancer detection and diagnosis is unclear. We aimed to investigate whether CNN could distinguish individuals with and without pancreatic cancer on CT, compared with radiologist interpretation.},
	language = {en},
	number = {6},
	urldate = {2025-04-20},
	journal = {The Lancet Digital Health},
	author = {Liu, Kao-Lang and Wu, Tinghui and Chen, Po-Ting and Tsai, Yuhsiang M and Roth, Holger and Wu, Ming-Shiang and Liao, Wei-Chih and Wang, Weichung},
	month = jun,
	year = {2020},
	pages = {e303--e313},
	file = {PDF:/Users/huangyijun/Zotero/storage/SHRWFA6U/Liu 等 - 2020 - Deep learning to distinguish pancreatic cancer tissue from non-cancerous pancreatic tissue a retros.pdf:application/pdf},
}

@article{noauthor_gastrointestinal_2021,
	title = {Gastrointestinal cancer classification and prognostication from histology using deep learning: {Systematic} review},
	volume = {155},
	issn = {0959-8049},
	shorttitle = {Gastrointestinal cancer classification and prognostication from histology using deep learning},
	url = {https://www.sciencedirect.com/science/article/pii/S0959804921004603},
	doi = {10.1016/j.ejca.2021.07.012},
	abstract = {Gastrointestinal cancers account for approximately 20\% of all cancer diagnoses and are responsible for 22.5\% of cancer deaths worldwide. Artificial in…},
	language = {zh-CN},
	urldate = {2025-04-20},
	journal = {European Journal of Cancer},
	month = sep,
	year = {2021},
	note = {Publisher: Pergamon},
	pages = {200--215},
	file = {Snapshot:/Users/huangyijun/Zotero/storage/SLBGNLYU/S0959804921004603.html:text/html},
}

@article{liu_deep_2024,
	title = {Deep learning‐based radiomics model can predict extranodal soft tissue metastasis in gastric cancer},
	volume = {51},
	issn = {0094-2405, 2473-4209},
	url = {https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.16647},
	doi = {10.1002/mp.16647},
	abstract = {Abstract
            
              Background
              The potential prognostic value of extranodal soft tissue metastasis (ESTM) has been confirmed by increasing studies about gastric cancer (GC). However, the gold standard of ESTM is determined by pathologic examination after surgery, and there are no preoperative methods for assessment of ESTM yet.
            
            
              Purpose
              This multicenter study aimed to develop a deep learning‐based radiomics model to preoperatively identify ESTM and evaluate its prognostic value.
            
            
              Methods
              
                A total of 959 GC patients were enrolled from two centers and split into a training cohort (
                N
                 = 551) and a test cohort (
                N
                 = 236) for ESTM evaluation. Additionally, an external survival cohort (
                N
                 = 172) was included for prognostic analysis. Four models were established based on clinical characteristics and multiphase computed tomography (CT) images for preoperative identification of ESTM, including a deep learning model, a hand‐crafted radiomic model, a clinical model, and a combined model. C‐index, decision curve, and calibration curve were utilized to assess the model performances. Survival analysis was conducted to explore the ability of stratifying overall survival (OS).
              
            
            
              Results
              
                The combined model showed good discrimination of the ESTM [C‐indices (95\% confidence interval, CI): 0.770 (0.729–0.812) and 0.761 (0.718–0.805) in training and test cohorts respectively], which outperformed deep learning model, radiomics model, and clinical model. The stratified analysis showed this model was not affected by patient's tumor size, the presence of lymphovascular invasion, and Lauren classification (
                p {\textless} 0.05
                ). Moreover, the model score showed strong consistency with the OS [C‐indices (95\%CI): 0.723 (0.658–0.789,
                p {\textless} 0.0001
                ) in the internal survival cohort and 0.715 (0.650–0.779,
                p {\textless} 0.0001
                ) in the external survival cohort]. More interestingly, univariate analysis showed the model score was significantly associated with occult distant metastasis (
                p {\textless} 0.05
                ) that was missed by preoperative diagnosis.
              
            
            
              Conclusions
              The model combining CT images and clinical characteristics had an impressive predictive ability of both ESTM and prognosis, which has the potential to serve as an effective complement to the preoperative TNM staging system.},
	language = {en},
	number = {1},
	urldate = {2025-04-20},
	journal = {Medical Physics},
	author = {Liu, Shengyuan and Deng, Jingyu and Dong, Di and Fang, Mengjie and Ye, Zhaoxiang and Hu, Yanfeng and Li, Hailin and Zhong, Lianzhen and Cao, Runnan and Zhao, Xun and Shang, Wenting and Li, Guoxin and Liang, Han and Tian, Jie},
	month = jan,
	year = {2024},
	pages = {267--277},
}

@article{bao_deep_2024,
	title = {Deep learning or radiomics based on {CT} for predicting the response of gastric cancer to neoadjuvant chemotherapy: a meta-analysis and systematic review},
	volume = {14},
	shorttitle = {Deep learning or radiomics based on {CT} for predicting the response of gastric cancer to neoadjuvant chemotherapy},
	url = {https://www.frontiersin.org/articles/10.3389/fonc.2024.1363812/full},
	urldate = {2025-04-20},
	journal = {Frontiers in Oncology},
	author = {Bao, Zhixian and Du, Jie and Zheng, Ya and Guo, Qinghong and Ji, Rui},
	year = {2024},
	note = {Publisher: Frontiers},
	pages = {1363812},
	file = {Available Version (via Google Scholar):/Users/huangyijun/Zotero/storage/T97IFYZ4/full.html:text/html},
}

@article{li_predicting_2024,
	title = {Predicting gastric cancer tumor mutational burden from histopathological images using multimodal deep learning},
	volume = {23},
	url = {https://academic.oup.com/bfg/article-abstract/23/3/228/7234267},
	number = {3},
	urldate = {2025-04-20},
	journal = {Briefings in Functional Genomics},
	author = {Li, Jing and Liu, Haiyan and Liu, Wei and Zong, Peijun and Huang, Kaimei and Li, Zibo and Li, Haigang and Xiong, Ting and Tian, Geng and Li, Chun},
	year = {2024},
	note = {Publisher: Oxford University Press},
	pages = {228--238},
	file = {Available Version (via Google Scholar):/Users/huangyijun/Zotero/storage/5Y5LE54W/Li 等 - 2024 - Predicting gastric cancer tumor mutational burden from histopathological images using multimodal dee.pdf:application/pdf},
}

@article{lee_ensemble_2024,
	title = {Ensemble deep learning model to predict lymphovascular invasion in gastric cancer},
	volume = {16},
	url = {https://www.mdpi.com/2072-6694/16/2/430},
	number = {2},
	urldate = {2025-04-20},
	journal = {Cancers},
	author = {Lee, Jonghyun and Cha, Seunghyun and Kim, Jiwon and Kim, Jung Joo and Kim, Namkug and Jae Gal, Seong Gyu and Kim, Ju Han and Lee, Jeong Hoon and Choi, Yoo-Duk and Kang, Sae-Ryung},
	year = {2024},
	note = {Publisher: MDPI},
	pages = {430},
	file = {Available Version (via Google Scholar):/Users/huangyijun/Zotero/storage/YPN7DLEK/Lee 等 - 2024 - Ensemble deep learning model to predict lymphovascular invasion in gastric cancer.pdf:application/pdf},
}

@article{zhang_early_2024,
	title = {Early gastric cancer detection and lesion segmentation based on deep learning and gastroscopic images},
	volume = {14},
	url = {https://www.nature.com/articles/s41598-024-58361-8},
	number = {1},
	urldate = {2025-04-20},
	journal = {Scientific Reports},
	author = {Zhang, Kezhi and Wang, Haibao and Cheng, Yaru and Liu, Hongyan and Gong, Qi and Zeng, Qian and Zhang, Tao and Wei, Guoqiang and Wei, Zhi and Chen, Dong},
	year = {2024},
	note = {Publisher: Nature Publishing Group UK London},
	pages = {7847},
	file = {Available Version (via Google Scholar):/Users/huangyijun/Zotero/storage/XQCLYNRA/Zhang 等 - 2024 - Early gastric cancer detection and lesion segmentation based on deep learning and gastroscopic image.pdf:application/pdf},
}

@article{haq_accurate_2024,
	title = {Accurate multiclassification and segmentation of gastric cancer based on a hybrid cascaded deep learning model with a vision transformer from endoscopic images},
	volume = {670},
	url = {https://www.sciencedirect.com/science/article/pii/S002002552400481X?casa_token=wYShtXnq3iUAAAAA:PtU7G9qybpU7LOQrn6jLIvFRaa-2ZeGuUBxB6Atgb_6hty1W-oxdp4BiBsEnTDre7BEYHDZdhds},
	urldate = {2025-04-20},
	journal = {Information Sciences},
	author = {Haq, Ejaz Ul and Yong, Qin and Yuan, Zhou and Jianjun, Huang and Haq, Rizwan Ul and Qin, Xuwen},
	year = {2024},
	note = {Publisher: Elsevier},
	pages = {120568},
}

@article{zheng_transformer-based_2024,
	title = {A transformer-based deep learning model for early prediction of lymph node metastasis in locally advanced gastric cancer after neoadjuvant chemotherapy using pretreatment {CT} images},
	volume = {75},
	url = {https://www.thelancet.com/journals/eclinm/article/PIIS2589-5370(24)00384-5/fulltext},
	urldate = {2025-04-20},
	journal = {EClinicalMedicine},
	author = {Zheng, Yunlin and Qiu, Bingjiang and Liu, Shunli and Song, Ruirui and Yang, Xianqi and Wu, Lei and Chen, Zhihong and Tuersun, Abudouresuli and Yang, Xiaotang and Wang, Wei},
	year = {2024},
	note = {Publisher: Elsevier},
}
