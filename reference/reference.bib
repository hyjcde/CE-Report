% Bibliography for CORTEX Cognitive Architecture Thesis

% Foundation Models and Large Language Models
@article{bommasani2021opportunities,
  title   = {On the opportunities and risks of foundation models},
  author  = {Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal = {arXiv preprint arXiv:2108.07258},
  year    = {2021}
}

@inproceedings{krizhevsky2012imagenet,
  title     = {Imagenet classification with deep convolutional neural networks},
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in neural information processing systems},
  volume    = {25},
  year      = {2012}
}

@inproceedings{vaswani2017attention,
  title     = {Attention is all you need},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle = {Advances in neural information processing systems},
  volume    = {30},
  year      = {2017}
}

@inproceedings{brown2020language,
  title     = {Language models are few-shot learners},
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle = {Advances in neural information processing systems},
  volume    = {33},
  pages     = {1877--1901},
  year      = {2020}
}

@article{openai2023gpt4,
  title   = {GPT-4 technical report},
  author  = {OpenAI},
  journal = {arXiv preprint arXiv:2303.08774},
  year    = {2023}
}

@article{touvron2023llama,
  title   = {Llama: Open and efficient foundation language models},
  author  = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal = {arXiv preprint arXiv:2302.13971},
  year    = {2023}
}

@article{chowdhery2022palm,
  title   = {PaLM: Scaling language modeling with pathways},
  author  = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal = {arXiv preprint arXiv:2204.02311},
  year    = {2022}
}

@article{wei2022chain,
  title   = {Chain-of-thought prompting elicits reasoning in large language models},
  author  = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal = {arXiv preprint arXiv:2201.11903},
  year    = {2022}
}

% LLM Agents and Tool Use
@article{xi2023rise,
  title   = {The rise and potential of large language model based agents: A survey},
  author  = {Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and others},
  journal = {arXiv preprint arXiv:2309.07864},
  year    = {2023}
}

@article{yao2022react,
  title   = {React: Synergizing reasoning and acting in language models},
  author  = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Avidan, Izhak and Edwards, Te-Yen and Le, Quoc V},
  journal = {arXiv preprint arXiv:2210.03629},
  year    = {2022}
}

@article{schick2023toolformer,
  title   = {Toolformer: Language models can teach themselves to use tools},
  author  = {Schick, Timo and Dwivedi-Yu, Jane and Dessi, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal = {arXiv preprint arXiv:2302.04761},
  year    = {2023}
}

@article{nakano2021webgpt,
  title   = {Webgpt: Browser-assisted question-answering with human feedback},
  author  = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal = {arXiv preprint arXiv:2112.09332},
  year    = {2021}
}

@misc{richards2023autogpt,
  title  = {AutoGPT: An autonomous GPT-4 experiment},
  author = {Richards, Toran Bruce and others},
  year   = {2023},
  url    = {https://github.com/Significant-Gravitas/AutoGPT}
}

@misc{chase2022langchain,
  title  = {LangChain: Building applications with LLMs through composability},
  author = {Chase, Harrison},
  year   = {2022},
  url    = {https://github.com/langchain-ai/langchain}
}

% Embodied AI and Robotics
@article{duan2022survey,
  title     = {A survey of embodied ai: From simulators to research tasks},
  author    = {Duan, Jiafei and Yu, Samson and Tan, Hui Li and Zhu, Hongyuan and Tan, Cheston},
  journal   = {IEEE Transactions on Emerging Topics in Computational Intelligence},
  volume    = {6},
  number    = {2},
  pages     = {230--244},
  year      = {2022},
  publisher = {IEEE}
}

@article{ahn2022can,
  title   = {Do as i can, not as i say: Grounding language in robotic affordances},
  author  = {Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and others},
  journal = {arXiv preprint arXiv:2204.01691},
  year    = {2022}
}

@article{brohan2023rt2,
  title   = {RT-2: Vision-language-action models transfer web knowledge to robotic control},
  author  = {Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Ho, Jasmine and others},
  journal = {arXiv preprint arXiv:2307.15818},
  year    = {2023}
}

@article{vemprala2023chatgpt,
  title   = {Chatgpt for robotics: Design principles and model abilities},
  author  = {Vemprala, Sai and Bonatti, Roger and Bucker, Arthur and Kapoor, Ashish},
  journal = {Microsoft Autonomous Systems and Robotics Research},
  year    = {2023}
}

@article{liu2023llm,
  title   = {LLM+P: Empowering large language models with optimal planning proficiency},
  author  = {Liu, Bo and Jiang, Yuqian and Zhang, Xiaohan and Liu, Qiang and Zhang, Shiqi and Biswas, Joydeep and Stone, Peter},
  journal = {arXiv preprint arXiv:2304.11477},
  year    = {2023}
}

% Symbol Grounding and Cognitive Science
@article{harnad1990symbol,
  title     = {The symbol grounding problem},
  author    = {Harnad, Stevan},
  journal   = {Physica D: Nonlinear Phenomena},
  volume    = {42},
  number    = {1-3},
  pages     = {335--346},
  year      = {1990},
  publisher = {Elsevier}
}

@article{searle1980minds,
  title     = {Minds, brains, and programs},
  author    = {Searle, John R},
  journal   = {Behavioral and brain sciences},
  volume    = {3},
  number    = {3},
  pages     = {417--424},
  year      = {1980},
  publisher = {Cambridge University Press}
}

@article{cangelosi2010integration,
  title     = {Integration of action and language knowledge: A roadmap for developmental robotics},
  author    = {Cangelosi, Angelo and Schlesinger, Matthew},
  journal   = {IEEE Transactions on Autonomous Mental Development},
  volume    = {2},
  number    = {3},
  pages     = {167--195},
  year      = {2010},
  publisher = {IEEE}
}

% Multimodal Learning and Vision-Language Models
@inproceedings{radford2021learning,
  title        = {Learning transferable visual models from natural language supervision},
  author       = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle    = {International conference on machine learning},
  pages        = {8748--8763},
  year         = {2021},
  organization = {PMLR}
}

@article{alayrac2022flamingo,
  title   = {Flamingo: a visual language model for few-shot learning},
  author  = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {35},
  pages   = {23716--23736},
  year    = {2022}
}

% Additional References for Physical World Understanding and Spatial Reasoning
@article{huang2023dynamic,
  title   = {Dynamic building information modeling for smart infrastructure management},
  author  = {Huang, Yijun and Others},
  journal = {Smart Cities},
  volume  = {10},
  number  = {2},
  pages   = {45--62},
  year    = {2023},
  note    = {This is a placeholder citation - to be updated with actual reference}
}

@article{chen2023spatialvlm,
  title   = {SpatialVLM: Endowing vision-language models with spatial reasoning capabilities},
  author  = {Chen, Boyuan and Xu, Fei and Xiang, Yuke and Kumar, Abhinav and Ichter, Brian and Majumdar, Arunkumar},
  journal = {arXiv preprint arXiv:2401.12168},
  year    = {2023}
}

@book{pearl2018book,
  title     = {The book of why: the new science of cause and effect},
  author    = {Pearl, Judea and Mackenzie, Dana},
  year      = {2018},
  publisher = {Basic books}
}

@article{kenton2021alignment,
  title   = {Alignment of language agents},
  author  = {Kenton, Zachary and Everitt, Tom and Weidinger, Laura and Gabriel, Iason and Mikulik, Vladimir and Irving, Geoffrey},
  journal = {arXiv preprint arXiv:2103.14659},
  year    = {2021}
}

% Existing references from the original file follow below
@misc{liu_swin_2021,
  title      = {Swin {Transformer}: {Hierarchical} {Vision} {Transformer} using {Shifted} {Windows}},
  shorttitle = {Swin {Transformer}},
  url        = {http://arxiv.org/abs/2103.14030},
  doi        = {10.48550/arXiv.2103.14030},
  abstract   = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with {\textbackslash}textbf\{S\}hifted {\textbackslash}textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at{\textasciitilde}{\textbackslash}url\{https://github.com/microsoft/Swin-Transformer\}.},
  urldate    = {2025-04-17},
  publisher  = {arXiv},
  author     = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  month      = aug,
  year       = {2021},
  note       = {arXiv:2103.14030 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
  file       = {Preprint PDF:/Users/huangyijun/Zotero/storage/ZJYRBLIX/Liu 等 - 2021 - Swin Transformer Hierarchical Vision Transformer using Shifted Windows.pdf:application/pdf;Snapshot:/Users/huangyijun/Zotero/storage/EY6A2CU5/2103.html:text/html}
}

@article{le_goallec_using_2022,
  title    = {Using deep learning to predict abdominal age from liver and pancreas magnetic resonance images},
  volume   = {13},
  issn     = {2041-1723},
  url      = {https://www.nature.com/articles/s41467-022-29525-9},
  doi      = {10.1038/s41467-022-29525-9},
  abstract = {Abstract
              
              With age, the prevalence of diseases such as fatty liver disease, cirrhosis, and type two diabetes increases. Approaches to both predict abdominal age and identify risk factors for accelerated abdominal age may ultimately lead to advances that will delay the onset of these diseases. We build an abdominal age predictor by training convolutional neural networks to predict abdominal age (or "AbdAge") from 45,552 liver magnetic resonance images [MRIs] and 36,784 pancreas MRIs (R-Squared = 73.3 ± 0.6; mean absolute error = 2.94 ± 0.03 years). Attention maps show that the prediction is driven by both liver and pancreas anatomical features, and surrounding organs and tissue. Abdominal aging is a complex trait, partially heritable (h\_g
              2
              = 26.3 ± 1.9\%), and associated with 16 genetic loci (e.g. in 
              PLEKHA1
              and
              EFEMP1
              ), biomarkers (e.g body impedance), clinical phenotypes (e.g, chest pain), diseases (e.g. hypertension), environmental (e.g smoking), and socioeconomic (e.g education, income) factors.},
  language = {en},
  number   = {1},
  urldate  = {2025-04-19},
  journal  = {Nature Communications},
  author   = {Le Goallec, Alan and Diai, Samuel and Collin, Sasha and Prost, Jean-Baptiste and Vincent, Théo and Patel, Chirag J.},
  month    = apr,
  year     = {2022},
  pages    = {1979},
  file     = {PDF:/Users/huangyijun/Zotero/storage/BYSJE6L6/Le Goallec 等 - 2022 - Using deep learning to predict abdominal age from liver and pancreas magnetic resonance images.pdf:application/pdf}
}

@article{yin_deep_2023,
  title      = {Deep learning for pancreatic diseases based on endoscopic ultrasound: {A} systematic review},
  volume     = {174},
  issn       = {1386-5056},
  shorttitle = {Deep learning for pancreatic diseases based on endoscopic ultrasound},
  url        = {https://www.sciencedirect.com/science/article/pii/S138650562300062X},
  doi        = {10.1016/j.ijmedinf.2023.105044},
  abstract   = {Background and aims
                Endoscopic ultrasonography (EUS) is one of the main examinations in pancreatic diseases. A series of the studies reported the application of deep learning (DL)-assisted EUS in the diagnosis of pancreatic diseases. This systematic review is to evaluate the role of DL algorithms in assisting EUS diagnosis of pancreatic diseases.
                Methods
                Literature search were conducted in PubMed and Semantic Scholar databases. Studies that developed DL models for pancreatic diseases based on EUS were eligible for inclusion. This review was conducted in accordance with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines and quality assessment of the included studies was performed according to the IJMEDI checklist.
                Results
                A total of 23 studies were enrolled into this systematic review, which could be categorized into three groups according to computer vision tasks: classification, detection and segmentation. Seventeen studies focused on the classification task, among which five studies developed simple neural network (NN) models while twelve studies constructed convolutional NN (CNN) models. Three studies were concerned the detection task and five studies were the segmentation task, all based on CNN architectures. All models presented in the studies performed well based on EUS images, videos or voice. According to the IJMEDI checklist, six studies were recognized as high-grade quality, with scores beyond 35 points.
                Conclusions
                DL algorithms show great potential in EUS images/videos/voice for pancreatic diseases. However, there is room for improvement such as sample sizes, multi-center cooperation, data preprocessing, model interpretability, and code sharing.},
  urldate    = {2025-04-20},
  journal    = {International Journal of Medical Informatics},
  author     = {Yin, Minyue and Liu, Lu and Gao, Jingwen and Lin, Jiaxi and Qu, Shuting and Xu, Wei and Liu, Xiaolin and Xu, Chunfang and Zhu, Jinzhou},
  month      = jun,
  year       = {2023},
  keywords   = {Convolutional neural networks, Deep learning, Endoscopic ultrasonography, Pancreatic diseases, Systematic review},
  pages      = {105044},
  file       = {ScienceDirect Snapshot:/Users/huangyijun/Zotero/storage/ENFGLSFL/S138650562300062X.html:text/html}
}

@article{chen_pancreatic_2023,
  title      = {Pancreatic {Cancer} {Detection} on {CT} {Scans} with {Deep} {Learning}: {A} {Nationwide} {Population}-based {Study}},
  volume     = {306},
  issn       = {0033-8419, 1527-1315},
  shorttitle = {Pancreatic {Cancer} {Detection} on {CT} {Scans} with {Deep} {Learning}},
  url        = {http://pubs.rsna.org/doi/10.1148/radiol.220152},
  doi        = {10.1148/radiol.220152},
  language   = {en},
  number     = {1},
  urldate    = {2025-04-20},
  journal    = {Radiology},
  author     = {Chen, Po-Ting and Wu, Tinghui and Wang, Pochuan and Chang, Dawei and Liu, Kao-Lang and Wu, Ming-Shiang and Roth, Holger R. and Lee, Po-Chang and Liao, Wei-Chih and Wang, Weichung},
  month      = jan,
  year       = {2023},
  pages      = {172--182},
  file       = {Full Text PDF:/Users/huangyijun/Zotero/storage/K6AI4S45/Chen 等 - 2023 - Pancreatic Cancer Detection on CT Scans with Deep Learning A Nationwide Population-based Study.pdf:application/pdf}
}

@article{si_fully_2021,
  title    = {Fully end-to-end deep-learning-based diagnosis of pancreatic tumors},
  volume   = {11},
  issn     = {1838-7640},
  url      = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7778580/},
  doi      = {10.7150/thno.52508},
  abstract = {Artificial intelligence can facilitate clinical decision making by considering massive amounts of medical imaging data. Various algorithms have been implemented for different clinical applications. Accurate diagnosis and treatment require reliable and interpretable data. For pancreatic tumor diagnosis, only 58.5\% of images from the First Affiliated Hospital and the Second Affiliated Hospital, Zhejiang University School of Medicine are used, increasing labor and time costs to manually filter out images not directly used by the diagnostic model., Methods: This study used a training dataset of 143,945 dynamic contrast-enhanced CT images of the abdomen from 319 patients. The proposed model contained four stages: image screening, pancreas location, pancreas segmentation, and pancreatic tumor diagnosis., Results: We established a fully end-to-end deep-learning model for diagnosing pancreatic tumors and proposing treatment. The model considers original abdominal CT images without any manual preprocessing. Our artificial-intelligence-based system achieved an area under the curve of 0.871 and a F1 score of 88.5\% using an independent testing dataset containing 107,036 clinical CT images from 347 patients. The average accuracy for all tumor types was 82.7\%, and the independent accuracies of identifying intraductal papillary mucinous neoplasm and pancreatic ductal adenocarcinoma were 100\% and 87.6\%, respectively. The average test time per patient was 18.6 s, compared with at least 8 min for manual reviewing. Furthermore, the model provided a transparent and interpretable diagnosis by producing saliency maps highlighting the regions relevant to its decision., Conclusions: The proposed model can potentially deliver efficient and accurate preoperative diagnoses that could aid the surgical management of pancreatic tumor.},
  number   = {4},
  urldate  = {2025-04-20},
  journal  = {Theranostics},
  author   = {Si, Ke and Xue, Ying and Yu, Xiazhen and Zhu, Xinpei and Li, Qinghai and Gong, Wei and Liang, Tingbo and Duan, Shumin},
  month    = jan,
  year     = {2021},
  pmid     = {33408793},
  pmcid    = {PMC7778580},
  pages    = {1982--1990},
  file     = {Full Text PDF:/Users/huangyijun/Zotero/storage/Y2R2BTSW/Si 等 - 2021 - Fully end-to-end deep-learning-based diagnosis of pancreatic tumors.pdf:application/pdf}
}

@article{liu_deep_2020,
  title      = {Deep learning to distinguish pancreatic cancer tissue from non-cancerous pancreatic tissue: a retrospective study with cross-racial external validation},
  volume     = {2},
  issn       = {25897500},
  shorttitle = {Deep learning to distinguish pancreatic cancer tissue from non-cancerous pancreatic tissue},
  url        = {https://linkinghub.elsevier.com/retrieve/pii/S2589750020300789},
  doi        = {10.1016/S2589-7500(20)30078-9},
  abstract   = {Background The diagnostic performance of CT for pancreatic cancer is interpreter-dependent, and approximately 40\% of tumours smaller than 2 cm evade detection. Convolutional neural networks (CNNs) have shown promise in image analysis, but the networks' potential for pancreatic cancer detection and diagnosis is unclear. We aimed to investigate whether CNN could distinguish individuals with and without pancreatic cancer on CT, compared with radiologist interpretation.},
  language   = {en},
  number     = {6},
  urldate    = {2025-04-20},
  journal    = {The Lancet Digital Health},
  author     = {Liu, Kao-Lang and Wu, Tinghui and Chen, Po-Ting and Tsai, Yuhsiang M and Roth, Holger and Wu, Ming-Shiang and Liao, Wei-Chih and Wang, Weichung},
  month      = jun,
  year       = {2020},
  pages      = {e303--e313},
  file       = {PDF:/Users/huangyijun/Zotero/storage/SHRWFA6U/Liu 等 - 2020 - Deep learning to distinguish pancreatic cancer tissue from non-cancerous pancreatic tissue a retros.pdf:application/pdf}
}

@article{noauthor_gastrointestinal_2021,
  title      = {Gastrointestinal cancer classification and prognostication from histology using deep learning: {Systematic} review},
  volume     = {155},
  issn       = {0959-8049},
  shorttitle = {Gastrointestinal cancer classification and prognostication from histology using deep learning},
  url        = {https://www.sciencedirect.com/science/article/pii/S0959804921004603},
  doi        = {10.1016/j.ejca.2021.07.012},
  abstract   = {Gastrointestinal cancers account for approximately 20\% of all cancer diagnoses and are responsible for 22.5\% of cancer deaths worldwide. Artificial in…},
  language   = {zh-CN},
  urldate    = {2025-04-20},
  journal    = {European Journal of Cancer},
  month      = sep,
  year       = {2021},
  note       = {Publisher: Pergamon},
  pages      = {200--215},
  file       = {Snapshot:/Users/huangyijun/Zotero/storage/SLBGNLYU/S0959804921004603.html:text/html}
}

@article{liu_deep_2024,
  title    = {Deep learning‐based radiomics model can predict extranodal soft tissue metastasis in gastric cancer},
  volume   = {51},
  issn     = {0094-2405, 2473-4209},
  url      = {https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.16647},
  doi      = {10.1002/mp.16647},
  abstract = {Abstract
              
              Background
              The potential prognostic value of extranodal soft tissue metastasis (ESTM) has been confirmed by increasing studies about gastric cancer (GC). However, the gold standard of ESTM is determined by pathologic examination after surgery, and there are no preoperative methods for assessment of ESTM yet.
              
              
              Purpose
              This multicenter study aimed to develop a deep learning‐based radiomics model to preoperatively identify ESTM and evaluate its prognostic value.
              
              
              Methods
              
              A total of 959 GC patients were enrolled from two centers and split into a training cohort (
              N
              = 551) and a test cohort (
              N
              = 236) for ESTM evaluation. Additionally, an external survival cohort (
              N
              = 172) was included for prognostic analysis. Four models were established based on clinical characteristics and multiphase computed tomography (CT) images for preoperative identification of ESTM, including a deep learning model, a hand‐crafted radiomic model, a clinical model, and a combined model. C‐index, decision curve, and calibration curve were utilized to assess the model performances. Survival analysis was conducted to explore the ability of stratifying overall survival (OS).
              
              
              
              Results
              
              The combined model showed good discrimination of the ESTM [C‐indices (95\% confidence interval, CI): 0.770 (0.729–0.812) and 0.761 (0.718–0.805) in training and test cohorts respectively], which outperformed deep learning model, radiomics model, and clinical model. The stratified analysis showed this model was not affected by patient's tumor size, the presence of lymphovascular invasion, and Lauren classification (
              p {\textless} 0.05
              ). Moreover, the model score showed strong consistency with the OS [C‐indices (95\%CI): 0.723 (0.658–0.789,
              p {\textless} 0.0001
              ) in the internal survival cohort and 0.715 (0.650–0.779,
              p {\textless} 0.0001
              ) in the external survival cohort]. More interestingly, univariate analysis showed the model score was significantly associated with occult distant metastasis (
              p {\textless} 0.05
              ) that was missed by preoperative diagnosis.
              
              
              
              Conclusions
              The model combining CT images and clinical characteristics had an impressive predictive ability of both ESTM and prognosis, which has the potential to serve as an effective complement to the preoperative TNM staging system.},
  language = {en},
  number   = {1},
  urldate  = {2025-04-20},
  journal  = {Medical Physics},
  author   = {Liu, Shengyuan and Deng, Jingyu and Dong, Di and Fang, Mengjie and Ye, Zhaoxiang and Hu, Yanfeng and Li, Hailin and Zhong, Lianzhen and Cao, Runnan and Zhao, Xun and Shang, Wenting and Li, Guoxin and Liang, Han and Tian, Jie},
  month    = jan,
  year     = {2024},
  pages    = {267--277}
}

@article{bao_deep_2024,
  title      = {Deep learning or radiomics based on {CT} for predicting the response of gastric cancer to neoadjuvant chemotherapy: a meta-analysis and systematic review},
  volume     = {14},
  shorttitle = {Deep learning or radiomics based on {CT} for predicting the response of gastric cancer to neoadjuvant chemotherapy},
  url        = {https://www.frontiersin.org/articles/10.3389/fonc.2024.1363812/full},
  urldate    = {2025-04-20},
  journal    = {Frontiers in Oncology},
  author     = {Bao, Zhixian and Du, Jie and Zheng, Ya and Guo, Qinghong and Ji, Rui},
  year       = {2024},
  note       = {Publisher: Frontiers},
  pages      = {1363812},
  file       = {Available Version (via Google Scholar):/Users/huangyijun/Zotero/storage/T97IFYZ4/full.html:text/html}
}

@article{li_predicting_2024,
  title   = {Predicting gastric cancer tumor mutational burden from histopathological images using multimodal deep learning},
  volume  = {23},
  url     = {https://academic.oup.com/bfg/article-abstract/23/3/228/7234267},
  number  = {3},
  urldate = {2025-04-20},
  journal = {Briefings in Functional Genomics},
  author  = {Li, Jing and Liu, Haiyan and Liu, Wei and Zong, Peijun and Huang, Kaimei and Li, Zibo and Li, Haigang and Xiong, Ting and Tian, Geng and Li, Chun},
  year    = {2024},
  note    = {Publisher: Oxford University Press},
  pages   = {228--238},
  file    = {Available Version (via Google Scholar):/Users/huangyijun/Zotero/storage/5Y5LE54W/Li 等 - 2024 - Predicting gastric cancer tumor mutational burden from histopathological images using multimodal dee.pdf:application/pdf}
}

@article{lee_ensemble_2024,
  title   = {Ensemble deep learning model to predict lymphovascular invasion in gastric cancer},
  volume  = {16},
  url     = {https://www.mdpi.com/2072-6694/16/2/430},
  number  = {2},
  urldate = {2025-04-20},
  journal = {Cancers},
  author  = {Lee, Jonghyun and Cha, Seunghyun and Kim, Jiwon and Kim, Jung Joo and Kim, Namkug and Jae Gal, Seong Gyu and Kim, Ju Han and Lee, Jeong Hoon and Choi, Yoo-Duk and Kang, Sae-Ryung},
  year    = {2024},
  note    = {Publisher: MDPI},
  pages   = {430},
  file    = {Available Version (via Google Scholar):/Users/huangyijun/Zotero/storage/YPN7DLEK/Lee 等 - 2024 - Ensemble deep learning model to predict lymphovascular invasion in gastric cancer.pdf:application/pdf}
}

@article{zhang_early_2024,
  title   = {Early gastric cancer detection and lesion segmentation based on deep learning and gastroscopic images},
  volume  = {14},
  url     = {https://www.nature.com/articles/s41598-024-58361-8},
  number  = {1},
  urldate = {2025-04-20},
  journal = {Scientific Reports},
  author  = {Zhang, Kezhi and Wang, Haibao and Cheng, Yaru and Liu, Hongyan and Gong, Qi and Zeng, Qian and Zhang, Tao and Wei, Guoqiang and Wei, Zhi and Chen, Dong},
  year    = {2024},
  note    = {Publisher: Nature Publishing Group UK London},
  pages   = {7847},
  file    = {Available Version (via Google Scholar):/Users/huangyijun/Zotero/storage/XQCLYNRA/Zhang 等 - 2024 - Early gastric cancer detection and lesion segmentation based on deep learning and gastroscopic image.pdf:application/pdf}
}

@article{haq_accurate_2024,
  title   = {Accurate multiclassification and segmentation of gastric cancer based on a hybrid cascaded deep learning model with a vision transformer from endoscopic images},
  volume  = {670},
  url     = {https://www.sciencedirect.com/science/article/pii/S002002552400481X?casa_token=wYShtXnq3iUAAAAA:PtU7G9qybpU7LOQrn6jLIvFRaa-2ZeGuUBxB6Atgb_6hty1W-oxdp4BiBsEnTDre7BEYHDZdhds},
  urldate = {2025-04-20},
  journal = {Information Sciences},
  author  = {Haq, Ejaz Ul and Yong, Qin and Yuan, Zhou and Jianjun, Huang and Haq, Rizwan Ul and Qin, Xuwen},
  year    = {2024},
  note    = {Publisher: Elsevier},
  pages   = {120568}
}

@article{zheng_transformer-based_2024,
  title   = {A transformer-based deep learning model for early prediction of lymph node metastasis in locally advanced gastric cancer after neoadjuvant chemotherapy using pretreatment {CT} images},
  volume  = {75},
  url     = {https://www.thelancet.com/journals/eclinm/article/PIIS2589-5370(24)00384-5/fulltext},
  urldate = {2025-04-20},
  journal = {EClinicalMedicine},
  author  = {Zheng, Yunlin and Qiu, Bingjiang and Liu, Shunli and Song, Ruirui and Yang, Xianqi and Wu, Lei and Chen, Zhihong and Tuersun, Abudouresuli and Yang, Xiaotang and Wang, Wei},
  year    = {2024},
  note    = {Publisher: Elsevier}
}

@article{bengio2003neural,
  title   = {A neural probabilistic language model},
  author  = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  journal = {Journal of machine learning research},
  volume  = {3},
  number  = {Feb},
  pages   = {1137--1155},
  year    = {2003}
}

@article{kaplan2020scaling,
  title   = {Scaling laws for neural language models},
  author  = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal = {arXiv preprint arXiv:2001.08361},
  year    = {2020}
}

@article{hoffmann2022training,
  title   = {Training compute-optimal large language models},
  author  = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal = {arXiv preprint arXiv:2203.15556},
  year    = {2022}
}

@article{yao2023tree,
  title   = {Tree of thoughts: Deliberate problem solving with large language models},
  author  = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L and Cao, Yuan and Narasimhan, Karthik},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {36},
  year    = {2023}
}

@article{wang2022self,
  title   = {Self-consistency improves chain of thought reasoning in language models},
  author  = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Sharan, Nazneen and Chowdhery, Aakanksha and Zhou, Denny},
  journal = {arXiv preprint arXiv:2203.11171},
  year    = {2022}
}

@article{li2023camel,
  title   = {Camel: Communicative agents for" mind" exploration of large scale language model society},
  author  = {Li, Guohao and Hammoud, Hasan Abed Al Kader and Itani, Hani and Khizbullin, Dmitrii and Ghanem, Bernard},
  journal = {arXiv preprint arXiv:2303.17760},
  year    = {2023}
}

@article{richards2023autogpt,
  title   = {AutoGPT: An autonomous GPT-4 experiment},
  author  = {Richards, Toran Bruce},
  journal = {GitHub repository},
  year    = {2023},
  url     = {https://github.com/Significant-Gravitas/Auto-GPT}
}

@article{grieves2014digital,
  title   = {Digital twin: Manufacturing excellence through virtual factory replication},
  author  = {Grieves, Michael},
  journal = {Digital Manufacturing},
  volume  = {1},
  number  = {1},
  pages   = {1--7},
  year    = {2014}
}

@article{rosen2015about,
  title   = {About the importance of autonomy and digital twins for the future of manufacturing},
  author  = {Rosen, Roland and Von Wichert, Georg and Lo, George and Bettenhausen, Kurt D},
  journal = {IFAC-PapersOnLine},
  volume  = {48},
  number  = {3},
  pages   = {567--572},
  year    = {2015}
}

@article{tao2018digital,
  title   = {Digital twin in industry: State-of-the-art},
  author  = {Tao, Fei and Cheng, Jiangfeng and Qi, Qinglin and Zhang, Meng and Zhang, He and Sui, Fangyuan},
  journal = {Future generation computer systems},
  volume  = {83},
  pages   = {721--735},
  year    = {2018}
}

@article{deng2021systematic,
  title   = {A systematic review of a digital twin city: A new pattern of urban governance toward smart cities},
  author  = {Deng, Taolue and Zhang, Kang and Shen, Zheng-Jun Max},
  journal = {Journal of management science and engineering},
  volume  = {6},
  number  = {2},
  pages   = {125--134},
  year    = {2021}
}

@article{rasheed2020digital,
  title   = {Digital twin: Values, challenges and enablers from a modeling perspective},
  author  = {Rasheed, Adil and San, Omer and Kvamsdal, Trond},
  journal = {IEEE Access},
  volume  = {8},
  pages   = {21980--22012},
  year    = {2020}
}

@article{wu2022digital,
  title   = {Digital twins in healthcare: A scoping review},
  author  = {Wu, Yue and Zhang, Kang and Zhang, Yifan},
  journal = {Journal of medical Internet research},
  volume  = {24},
  number  = {9},
  pages   = {e35772},
  year    = {2022}
}

@article{liu2021review,
  title   = {Review of digital twin about concepts, technologies, and industrial applications},
  author  = {Liu, Meng and Fang, Shuiliang and Dong, Huiyong and Xu, Cunzhi},
  journal = {Journal of manufacturing systems},
  volume  = {58},
  pages   = {346--361},
  year    = {2021}
}

@article{kritzinger2018digital,
  title   = {Digital twin in manufacturing: A categorical literature review and classification},
  author  = {Kritzinger, Werner and Karner, Matthias and Traar, Georg and Henjes, Jan and Sihn, Wilfried},
  journal = {IFAC-PapersOnLine},
  volume  = {51},
  number  = {11},
  pages   = {1016--1022},
  year    = {2018}
}

@article{fuller2020digital,
  title   = {Digital twin: Enabling technologies, challenges and open research},
  author  = {Fuller, Aidan and Fan, Zhong and Day, Charles and Barlow, Chris},
  journal = {IEEE Access},
  volume  = {8},
  pages   = {108952--108971},
  year    = {2020}
}

@article{werner2021digital,
  title   = {Digital twins of manufacturing systems as a base for machine learning},
  author  = {Werner, Alexander and Zimmermann, Nils and Lentes, Joachim},
  journal = {Manufacturing Letters},
  volume  = {25},
  pages   = {73--76},
  year    = {2021}
}

@article{lu2020digital,
  title   = {Digital twin-driven smart manufacturing: Connotation, reference model, applications and research issues},
  author  = {Lu, Yuqian and Liu, Chao and Kevin, I and Wang, Kevin and Huang, Huiyue and Xu, Xun},
  journal = {Robotics and computer-integrated manufacturing},
  volume  = {61},
  pages   = {101837},
  year    = {2020}
}

@article{hartmann2018digital,
  title   = {Digital twin applications in the context of industry 4.0},
  author  = {Hartmann, Dominik and Van der Auweraer, Herman},
  journal = {IFAC-PapersOnLine},
  volume  = {51},
  number  = {11},
  pages   = {1678--1682},
  year    = {2018}
}

@article{sepasgozar2021digital,
  title   = {Digital twin and web-based virtual gaming technologies for online education: A case of construction management and engineering},
  author  = {Sepasgozar, Samad ME},
  journal = {Applied Sciences},
  volume  = {11},
  number  = {9},
  pages   = {4017},
  year    = {2021}
}

@article{laird2012soar,
  title   = {The Soar cognitive architecture},
  author  = {Laird, John E.},
  journal = {MIT press},
  year    = {2012}
}

@article{anderson2004integrated,
  title   = {An integrated theory of the mind},
  author  = {Anderson, John R and Bothell, Daniel and Byrne, Michael D and Douglass, Scott and Lebiere, Christian and Qin, Yulin},
  journal = {Psychological review},
  volume  = {111},
  number  = {4},
  pages   = {1036},
  year    = {2004}
}

@article{sun2007hybrid,
  title   = {The Cambridge handbook of computational psychology},
  author  = {Sun, Ron},
  journal = {Cambridge University Press},
  year    = {2007}
}

@book{pfeifer2006body,
  title     = {How the body shapes the way we think: a new view of intelligence},
  author    = {Pfeifer, Rolf and Bongard, Josh},
  year      = {2006},
  publisher = {MIT press}
}

@article{brooks1991intelligence,
  title   = {Intelligence without representation},
  author  = {Brooks, Rodney A},
  journal = {Artificial intelligence},
  volume  = {47},
  number  = {1-3},
  pages   = {139--159},
  year    = {1991}
}

@article{clark2013whatever,
  title   = {Whatever next? Predictive brains, situated agents, and the future of cognitive science},
  author  = {Clark, Andy},
  journal = {Behavioral and brain sciences},
  volume  = {36},
  number  = {3},
  pages   = {181--204},
  year    = {2013}
}

@article{breazeal2016social,
  title   = {Social robotics},
  author  = {Breazeal, Cynthia},
  journal = {Annual review of control, robotics, and autonomous systems},
  volume  = {1},
  pages   = {423--452},
  year    = {2016}
}

@article{li2023blip2,
  title   = {BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author  = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  journal = {arXiv preprint arXiv:2301.12597},
  year    = {2023}
}

@article{wu2023survey,
  title   = {A survey of large language models},
  author  = {Wu, Wayne Xin Zhao and Zhou, Kun and Li, Junyi and Tang, Jiayi and Wang, Xin and Liu, Yupeng and Wen, Ji-Rong},
  journal = {arXiv preprint arXiv:2303.18223},
  year    = {2023}
}

@article{hendrycks2023overview,
  title   = {An overview of catastrophic AI risks},
  author  = {Hendrycks, Dan and Mazeika, Mantas and Woodside, Thomas},
  journal = {arXiv preprint arXiv:2306.12001},
  year    = {2023}
}

@article{garcez2023neurosymbolic,
  title   = {Neurosymbolic AI: The 3rd wave},
  author  = {Garcez, Artur d'Avila and Lamb, Luis C},
  journal = {Artificial Intelligence Review},
  volume  = {56},
  number  = {11},
  pages   = {12387--12406},
  year    = {2023}
}

@article{davis2015commonsense,
  title   = {Commonsense reasoning and commonsense knowledge in artificial intelligence},
  author  = {Davis, Ernest and Marcus, Gary},
  journal = {Communications of the ACM},
  volume  = {58},
  number  = {9},
  pages   = {92--103},
  year    = {2015}
}

@article{yi2018neural,
  title   = {Neural-symbolic VQA: Disentangling reasoning from vision and language understanding},
  author  = {Yi, Kexin and Wu, Jiajun and Gan, Chuang and Torralba, Antonio and Kohli, Pushmeet and Tenenbaum, Joshua B},
  journal = {Advances in neural information processing systems},
  volume  = {31},
  year    = {2018}
}

@article{lake2017building,
  title   = {Building machines that learn and think like people},
  author  = {Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and Gershman, Samuel J},
  journal = {Behavioral and brain sciences},
  volume  = {40},
  pages   = {e253},
  year    = {2017}
}

@article{lim2020real,
  title   = {Real-time detection of stealthy DDoS attacks using machine learning},
  author  = {Lim, Haw-Yun and Tham, Chen-Khong and Gao, Chuanxiong},
  journal = {Journal of Network and Computer Applications},
  volume  = {173},
  pages   = {102862},
  year    = {2020}
}

@article{li2020survey,
  title   = {A survey on deep learning for named entity recognition},
  author  = {Li, Jing and Sun, Aixin and Han, Jianglei and Li, Chenliang},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume  = {34},
  number  = {1},
  pages   = {50--70},
  year    = {2020}
}

@article{wang2022survey,
  title   = {A survey on edge computing systems and tools},
  author  = {Wang, Sheng and Zhao, Yiwei and Xu, Jie and Wang, Jinping and Hsu, Chia-Hung and Lin, Xiaoming},
  journal = {Proceedings of the IEEE},
  volume  = {107},
  number  = {8},
  pages   = {1537--1562},
  year    = {2022}
}

@article{schwartz2020green,
  title   = {Green AI},
  author  = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A and Etzioni, Oren},
  journal = {Communications of the ACM},
  volume  = {63},
  number  = {12},
  pages   = {54--63},
  year    = {2020}
}

@article{dulac2019challenges,
  title   = {Challenges in safety engineering of ML components in autonomous driving},
  author  = {Dulac-Arnold, Gabriel and Mankowitz, Daniel and Hester, Todd},
  journal = {arXiv preprint arXiv:1904.07842},
  year    = {2019}
}

@article{zhao2020sim,
  title   = {Sim-to-real transfer in deep reinforcement learning for robotics: a survey},
  author  = {Zhao, Wenshuai and Queralta, Jorge Pe{\~n}a and Westerlund, Tomi},
  journal = {arXiv preprint arXiv:2009.13303},
  year    = {2020}
}

@article{amodei2016concrete,
  title   = {Concrete problems in AI safety},
  author  = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  journal = {arXiv preprint arXiv:1606.06565},
  year    = {2016}
}

@article{koh2021wilds,
  title   = {WILDS: A benchmark of in-the-wild distribution shifts},
  author  = {Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irene and others},
  journal = {arXiv preprint arXiv:2012.07421},
  year    = {2021}
}

% Building Inspection and Digital Twin References
@article{zhang2024automated,
  title     = {Automated High-Precision Digital Twin Modeling of Building Façade Defects with GeoBIM-assisted Registration},
  author    = {Zhang, Jihan and Zhao, Benyun and Yang, Guidong and Zhou, Xunkuai and Huang, Yijun and Gao, Chuanxiang and Chen, Xi and Chen, Ben M},
  year      = {2024},
  journal   = {SSRN Electronic Journal},
  doi       = {10.2139/ssrn.4869787},
  url       = {https://ssrn.com/abstract=4869787},
  publisher = {Elsevier BV}
}

@article{spencer2019advances,
  title     = {Advances in computer vision-based civil infrastructure inspection and monitoring},
  author    = {Spencer Jr, Billie F and Hoskere, Vedhus and Narazaki, Yasutaka},
  journal   = {Engineering},
  volume    = {5},
  number    = {2},
  pages     = {199--222},
  year      = {2019},
  publisher = {Elsevier}
}

@article{zhang2023automated,
  title   = {Automated UAV image-to-BIM registration for building façade inspection using improved generalised Hough transform},
  author  = {Zhang, Cheng and Wang, Feng and Zou, Yang and Dimyadi, Johannes and Guo, Brian HW and Hou, Lei},
  journal = {Automation in Construction},
  volume  = {153},
  pages   = {104957},
  year    = {2023}
}

@article{li2024single,
  title   = {Single drone-based 3D reconstruction approach to improve public engagement in conservation of heritage buildings: A case of hakka tulou},
  author  = {Li, Qingxiang and Yang, Guidong and Gao, Chuanxiang and Huang, Yijun and Zhang, Jihan and Huang, Dongyue and Zhao, Benyun and Chen, Xi and Chen, Ben M},
  journal = {Journal of Building Engineering},
  volume  = {87},
  pages   = {108954},
  year    = {2024}
}

@article{wang2023augmented,
  title   = {Augmented language models: a survey},
  author  = {Wang, Yihong and others},
  journal = {arXiv preprint arXiv:2302.07842},
  year    = {2023}
}

@article{chen2023automated,
  title   = {Automatic concrete defect detection and reconstruction by aligning aerial images onto semantic-rich building information model},
  author  = {Chen, Junjie and Lu, Weisheng and Lou, Jinfeng},
  journal = {Computer-Aided Civil and Infrastructure Engineering},
  volume  = {38},
  number  = {8},
  pages   = {1079--1098},
  year    = {2023}
}

@article{zhang2022integrating,
  title     = {Integrating UAV and BIM for automated visual building inspection: a systematic review and conceptual framework},
  author    = {Zhang, C and Zou, Y and Dimyadi, J},
  journal   = {IOP Conference Series: Earth and Environmental Science},
  volume    = {1101},
  number    = {6},
  pages     = {062030},
  year      = {2022},
  publisher = {IOP Publishing}
}

@article{bruno2018historic,
  title     = {Historic Building Information Modelling: performance assessment for diagnosis-aided information modelling and management},
  author    = {Bruno, Silvana and De Fino, Mariella and Fatiguso, Fabio},
  journal   = {Automation in Construction},
  volume    = {86},
  pages     = {256-276},
  year      = {2018},
  publisher = {Elsevier}
}

@article{ismail2021how,
  title     = {How BIM systems affect maintaining IBS building},
  author    = {Ismail, Zul-Atfi},
  journal   = {Facilities},
  volume    = {39},
  number    = {3/4},
  pages     = {196-214},
  year      = {2021},
  publisher = {Emerald Publishing Limited}
}

@article{hamdan2021semantic,
  title     = {A semantic modeling approach for the automated detection and interpretation of structural damage},
  author    = {Hamdan, Ahmad-Hasan and Taraben, Jakob and Helmrich, Mathias and Mansperger, Tobias and Morgenthal, Guido and Scherer, Raimar J},
  journal   = {Automation in Construction},
  volume    = {128},
  pages     = {103742},
  year      = {2021},
  publisher = {Elsevier}
}

@article{tsilimantou2020gis,
  title     = {GIS and BIM as integrated digital environments for modeling and monitoring of historic buildings},
  author    = {Tsilimantou, Elisavet and Delegou, Ekaterini T and Nikitakos, Ioannis A and Ioannidis, Charalabos and Moropoulou, Antonia},
  journal   = {Applied Sciences},
  volume    = {10},
  number    = {3},
  pages     = {1078},
  year      = {2020},
  publisher = {MDPI}
}

@article{chen2021geo,
  title     = {Geo-registering UAV-captured close-range images to GIS-based spatial model for building façade inspections},
  author    = {Chen, Keyu and Reichard, Georg and Akanmu, Abiola and Xu, Xiaowei},
  journal   = {Automation in Construction},
  volume    = {122},
  pages     = {103484},
  year      = {2021},
  publisher = {Elsevier}
}

@article{nepal2021towards,
  title     = {Towards an integrated approach to infrastructure damage assessment in the aftermath of natural hazards},
  author    = {Nepal, Madhav Prasad and Hon, Carol and Lee, Jia-Rong and Xiang, Zixiang},
  journal   = {Buildings},
  volume    = {11},
  number    = {8},
  pages     = {345},
  year      = {2021},
  publisher = {MDPI}
}

@article{ribeiro2020remote,
  title     = {Remote inspection of RC structures using unmanned aerial vehicles and heuristic image processing},
  author    = {Ribeiro, Diogo and Santos, Rui and Shibasaki, Atsuhiko and Montenegro, Pedro and Carvalho, Helder and Cal{\c{c}}ada, Rui},
  journal   = {Engineering Failure Analysis},
  volume    = {117},
  pages     = {104796},
  year      = {2020},
  publisher = {Elsevier}
}

@article{tan2021automatic,
  title     = {Automatic inspection data collection of building surface based on BIM and UAV},
  author    = {Tan, Yu and Li, Shaohua and Liu, Huiyuan and Chen, Pengfei and Zhou, Zena},
  journal   = {Automation in Construction},
  volume    = {131},
  pages     = {103883},
  year      = {2021},
  publisher = {Elsevier}
}

@article{liu2021integrating,
  title     = {Integrating Building Information Model and Augmented Reality for Drone-Based Building Inspection},
  author    = {Liu, Dingxiong and Xia, Xinyi and Chen, Jianfei and Li, Shaohua},
  journal   = {Journal of Computing in Civil Engineering},
  volume    = {35},
  number    = {2},
  pages     = {04020069},
  year      = {2021},
  publisher = {American Society of Civil Engineers}
}

% RAG and LLM References
@article{lewis2020retrieval,
  title   = {Retrieval-augmented generation for knowledge-intensive NLP tasks},
  author  = {Lewis, Patrick and others},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  pages   = {9459--9474},
  year    = {2020}
}

@article{fan2023retrieval,
  title   = {Retrieval-augmented generation for AI-generated content: A survey},
  author  = {Fan, Wenqi and others},
  journal = {arXiv preprint arXiv:2312.10997},
  year    = {2023}
}

@article{gao2023survey,
  title   = {A survey on hallucination in large language models},
  author  = {Gao, Lingyu and others},
  journal = {arXiv preprint arXiv:2305.10090},
  year    = {2023}
}

@article{ding2024survey,
  title   = {A Survey on RAG Meets LLMs: Towards Retrieval-Augmented Large Language Models},
  author  = {Ding, Yujuan and others},
  journal = {arXiv preprint arXiv:2405.06211},
  year    = {2024}
}

@inproceedings{borgeaud2022improving,
  title        = {Improving language models by retrieving from trillions of tokens},
  author       = {Borgeaud, Sebastian and others},
  booktitle    = {International Conference on Machine Learning},
  pages        = {1119--1137},
  year         = {2022},
  organization = {PMLR}
}

@inproceedings{izacard2021leveraging,
  title     = {Leveraging passage retrieval with generative models for open domain question answering},
  author    = {Izacard, Gautier and Grave, Edouard},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  pages     = {874--880},
  year      = {2021}
}
